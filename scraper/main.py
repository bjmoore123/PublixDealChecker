"""
scraper/main.py  (v6 - serverless Lambda)

Lambda entry point. Triggered by EventBridge weekly.
Writes a scrape-job summary record to DynamoDB after each run.
Uses per-user notify_email for alert delivery.

v6 additions:
  - history_deals()  â€” writes a dated weekly snapshot to deal-history table
  - update_corpus()  â€” merges deal titles/brands into global corpus for autocomplete
"""

import json
import os
import boto3
import hmac
import hashlib
import resend
from datetime import date, datetime, timezone, timedelta
from decimal import Decimal

from scraper import get_deals
from matcher import match_deals_for_user

# â”€â”€ Config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

AWS_REGION      = os.environ.get("PDC_REGION", os.environ.get("AWS_REGION", "us-east-1"))
USERS_TABLE     = os.environ["USERS_TABLE"]
DEALS_TABLE     = os.environ["DEALS_TABLE"]
SCRAPE_LOGS_TBL = os.environ.get("SCRAPE_LOGS_TABLE", "publix-deal-checker-scrape-logs")
HISTORY_TABLE   = os.environ.get("HISTORY_TABLE", "")
CORPUS_TABLE    = os.environ.get("CORPUS_TABLE", "")

resend.api_key  = os.environ["RESEND_API_KEY"]
FRONTEND_URL    = os.environ.get("FRONTEND_URL", "")
API_URL         = os.environ.get("API_URL", "")        # base URL for unsubscribe endpoint
UNSUB_SECRET    = (os.environ.get("UNSUB_SECRET", "") or "").strip()
RESEND_FROM    = (
    f"{os.environ.get('RESEND_FROM_NAME','Publix Alerts')} "
    f"<{os.environ.get('RESEND_FROM_ADDR','onboarding@resend.dev')}>"
)

dynamodb        = boto3.resource("dynamodb", region_name=AWS_REGION)
users_table     = dynamodb.Table(USERS_TABLE)
deals_table     = dynamodb.Table(DEALS_TABLE)
scrape_logs_tbl = dynamodb.Table(SCRAPE_LOGS_TBL)
APP_LOGS_TBL    = os.environ.get("APP_LOGS_TABLE", "publix-deal-checker-app-logs")
app_logs_tbl    = dynamodb.Table(APP_LOGS_TBL)


# â”€â”€ DynamoDB helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def to_python(obj):
    if isinstance(obj, Decimal):
        return int(obj) if obj == int(obj) else float(obj)
    if isinstance(obj, dict):  return {k: to_python(v) for k, v in obj.items()}
    if isinstance(obj, list):  return [to_python(i) for i in obj]
    return obj


def _log_email(to: str, subject: str, ok: bool, user: str = "", message: str = ""):
    """Log an email delivery attempt to app_logs DynamoDB table."""
    try:
        import secrets as _sec
        now = datetime.now(timezone.utc).isoformat()
        app_logs_tbl.put_item(Item={
            "log_id":  f"{now}#{_sec.token_hex(4)}",
            "ts":      now,
            "source":  "email",
            "level":   "info" if ok else "error",
            "ok":      ok,
            "to":      to,
            "subject": subject[:120],
            "trigger": "scheduled",
            "user":    user,
            "message": message[:200],
        })
    except Exception:
        pass


def get_all_users() -> list[dict]:
    items, resp = [], users_table.scan(ProjectionExpression="email, prefs")
    items.extend(resp.get("Items", []))
    while "LastEvaluatedKey" in resp:
        resp = users_table.scan(
            ProjectionExpression="email, prefs",
            ExclusiveStartKey=resp["LastEvaluatedKey"],
        )
        items.extend(resp.get("Items", []))
    return [to_python(u) for u in items]


CHUNK_SIZE = 200  # deals per DynamoDB row (~200 rows * ~200 bytes = ~40KB per chunk, well under 400KB)

def cache_deals(store_id: str, deals: list[dict]):
    fetched_at = datetime.now(timezone.utc).isoformat()
    ttl_epoch  = int((datetime.now(timezone.utc) + timedelta(days=10)).timestamp())
    chunks = [deals[i:i+CHUNK_SIZE] for i in range(0, max(len(deals), 1), CHUNK_SIZE)]
    num_chunks = len(chunks)

    # Write each chunk as store_id#N
    for i, chunk in enumerate(chunks):
        deals_table.put_item(Item={
            "store_id":    f"{store_id}#{i}",
            "fetched_at":  fetched_at,
            "deals":       json.dumps(chunk),
            "count":       len(chunk),
            "num_chunks":  num_chunks,
            "chunk_index": i,
            "expires_at":  ttl_epoch,
        })

    # Write an index row at store_id (no deals, just metadata)
    deals_table.put_item(Item={
        "store_id":   store_id,
        "fetched_at": fetched_at,
        "count":      len(deals),
        "num_chunks": num_chunks,
        "expires_at": ttl_epoch,
    })


def _week_start(deals: list[dict]) -> str:
    """
    Derive the Monday of the current ad week from the first deal's valid_from date.
    Falls back to the Monday of today if no valid_from is present or parseable.
    """
    for d in deals:
        vf = d.get("valid_from", "")
        if vf:
            try:
                # valid_from may be "Jan 23, 2026" or "2026-01-23" â€” try both
                for fmt in ("%b %d, %Y", "%Y-%m-%d", "%m/%d/%Y"):
                    try:
                        dt = datetime.strptime(vf.strip(), fmt)
                        # Roll back to Monday
                        monday = dt - timedelta(days=dt.weekday())
                        return monday.strftime("%Y-%m-%d")
                    except ValueError:
                        continue
            except Exception:
                pass
    # Fallback: Monday of current week
    today = date.today()
    monday = today - timedelta(days=today.weekday())
    return monday.strftime("%Y-%m-%d")


def _slim_deal(d: dict) -> dict:
    """Return a lightweight deal shape for history storage (strips large fields)."""
    return {
        "id":          d.get("id", ""),
        "title":       d.get("title", ""),
        "savings":     d.get("savings", ""),
        "saving_type": d.get("saving_type", ""),
        "is_bogo":     d.get("is_bogo", False),
        "department":  d.get("department", ""),
        "brand":       d.get("brand", ""),
    }


def history_deals(store_id: str, deals: list[dict], week_start: str):
    """
    Write a weekly snapshot of deals to the deal-history table.
    Key pattern: store_id#YYYY-MM-DD (index row) + store_id#YYYY-MM-DD#0, #1... (chunks)
    TTL: 52 weeks from now.
    """
    if not HISTORY_TABLE:
        return

    history_tbl = dynamodb.Table(HISTORY_TABLE)
    slim_deals  = [_slim_deal(d) for d in deals]
    fetched_at  = datetime.now(timezone.utc).isoformat()
    ttl_epoch   = int((datetime.now(timezone.utc) + timedelta(weeks=52)).timestamp())
    base_key    = f"{store_id}#{week_start}"

    chunks     = [slim_deals[i:i+CHUNK_SIZE] for i in range(0, max(len(slim_deals), 1), CHUNK_SIZE)]
    num_chunks = len(chunks)

    for i, chunk in enumerate(chunks):
        history_tbl.put_item(Item={
            "store_id":    f"{base_key}#{i}",
            "fetched_at":  fetched_at,
            "deals":       json.dumps(chunk),
            "count":       len(chunk),
            "num_chunks":  num_chunks,
            "chunk_index": i,
            "expires_at":  ttl_epoch,
        })

    # Index row â€” no deals, just metadata
    history_tbl.put_item(Item={
        "store_id":   base_key,
        "week":       week_start,
        "fetched_at": fetched_at,
        "count":      len(slim_deals),
        "num_chunks": num_chunks,
        "expires_at": ttl_epoch,
    })

    print(f"  History snapshot written: {base_key} ({len(slim_deals)} deals, {num_chunks} chunk(s))")


def update_corpus(deals: list[dict]):
    """
    Merge deal titles and brand strings from this week's deals into the
    global corpus row (key="global") using DynamoDB ADD on a StringSet.
    The corpus feeds the List tab autocomplete with cross-week deal names.
    No TTL â€” corpus grows indefinitely, never shrinks.
    """
    if not CORPUS_TABLE:
        return

    corpus_tbl = dynamodb.Table(CORPUS_TABLE)

    # Collect all non-empty titles and brands
    new_terms = set()
    for d in deals:
        title = (d.get("title") or "").strip()
        brand = (d.get("brand") or "").strip()
        if title:
            new_terms.add(title)
        if brand:
            new_terms.add(brand)

    if not new_terms:
        return

    try:
        corpus_tbl.update_item(
            Key={"corpus_id": "global"},
            UpdateExpression="ADD terms :t SET updated_at = :u",
            ExpressionAttributeValues={
                ":t": new_terms,
                ":u": datetime.now(timezone.utc).isoformat(),
            },
        )
        print(f"  Corpus updated: {len(new_terms)} term(s) merged")
    except Exception as e:
        print(f"  WARNING: corpus update failed: {e}")


def write_scrape_log(log: dict):
    """Persist scrape job summary to DynamoDB with a 1-year TTL."""
    try:
        log["expires_at"] = int((datetime.now(timezone.utc) + timedelta(days=365)).timestamp())
        scrape_logs_tbl.put_item(Item=log)
    except Exception as e:
        print(f"WARNING: failed to write scrape log: {e}")


# â”€â”€ Email â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def _unsub_token(email: str) -> str:
    """HMAC-SHA256 of email â€” matches api.py logic. Returns hex string."""
    return hmac.new(
        UNSUB_SECRET.encode(),
        email.lower().strip().encode(),
        hashlib.sha256,
    ).hexdigest()

def _unsub_url(account_email: str) -> str:
    """Build the full unsubscribe URL for a given account email."""
    if not UNSUB_SECRET or not API_URL:
        return ""
    import urllib.parse
    token = _unsub_token(account_email)
    params = urllib.parse.urlencode({"email": account_email, "token": token})
    return f"{API_URL}/user/unsubscribe?{params}"

def build_html(matches, store_id, store_name, frontend_url="", account_email=""):
    today = date.today().strftime("%B %d, %Y")
    rows  = ""
    for m in matches:
        valid = ""
        if m.get("valid_from") or m.get("valid_thru"):
            valid = f'<div style="font-size:11px;color:#999;">Valid {m.get("valid_from","")}â€“{m.get("valid_thru","")}</div>'
        coupon_link = ""
        if m.get("coupon_id"):
            coupon_link = f'<div><a href="https://www.publix.com/savings/digital-coupons?cid={m["coupon_id"]}" style="font-size:11px;color:#8b5cf6;">âœ‚ï¸ Clip Coupon</a></div>'
        rows += f"""
        <tr style="border-bottom:1px solid #eee;">
          <td style="padding:10px 8px;width:40px;text-align:center;font-size:22px;">ğŸ›’</td>
          <td style="padding:10px 8px;">
            <div style="font-size:10px;font-weight:700;color:#1a6b3c;text-transform:uppercase;letter-spacing:.07em;margin-bottom:2px;">{m.get('my_item','')}</div>
            <div style="font-weight:600;">{m['deal_name']}</div>
            {f'<div style="font-size:12px;color:#555;">{m["description"]}</div>' if m.get('description') else ''}
            {f'<div style="font-size:11px;color:#999;">{m["fine_print"]}</div>' if m.get('fine_print') else ''}
            {valid}
            {coupon_link}
          </td>
          <td style="padding:10px 8px;text-align:right;white-space:nowrap;">
            <div style="font-size:20px;font-weight:700;color:#1a6b3c;">{m.get('savings','')}</div>
            {f'<div style="font-size:12px;color:#e76f51;">{m["save_line"]}</div>' if m.get('save_line') else ''}
          </td>
        </tr>"""

    unsub_url = _unsub_url(account_email) if account_email else ""
    unsub_section = (
        f'<div style="margin-top:8px;font-size:12px;color:#aaa;">'
        f'Don\'t want these alerts? '
        f'<a href="{unsub_url}" style="color:#aaa;">Unsubscribe</a>'
        f'</div>'
    ) if unsub_url else ""

    return f"""<html><body style="font-family:Arial,sans-serif;max-width:680px;margin:0 auto;color:#333;">
      <div style="background:#1a6b3c;color:white;padding:20px 24px;border-radius:8px 8px 0 0;">
        <h2 style="margin:0;">&#x1F6D2; Publix Deal Alert</h2>
        <p style="margin:4px 0 0;opacity:.85;">{today} &nbsp;&middot;&nbsp; {store_name or f'Store #{store_id}'} &nbsp;&middot;&nbsp; {len(matches)} item(s) on sale</p>
      </div>
      <table style="width:100%;border-collapse:collapse;background:white;border:1px solid #ddd;border-top:none;">
        <thead><tr style="background:#f8f8f8;font-size:11px;color:#888;text-transform:uppercase;">
          <th style="padding:8px;"></th>
          <th style="padding:8px;text-align:left;">Item</th>
          <th style="padding:8px;text-align:right;">Price</th>
        </tr></thead>
        <tbody>{rows}</tbody>
      </table>
      <div style="padding:16px;background:#f0f7f3;border:1px solid #ddd;border-top:none;border-radius:0 0 8px 8px;text-align:center;">
        <a href="{frontend_url or 'https://www.publix.com/savings/weekly-ad/view-all'}"
           style="background:#1a6b3c;color:white;padding:10px 24px;border-radius:4px;text-decoration:none;font-weight:600;">
          View My Matches &#x2197;
        </a>
        {unsub_section}
      </div>
    </body></html>"""


def send_alert(notify_email: str, matches: list[dict], store_id: str, store_name: str,
               account_email: str = ""):
    today   = date.today().strftime("%B %d")
    subject = f"ğŸ›’ Publix Deals â€” {len(matches)} item(s) on sale ({today})"
    try:
        resend.Emails.send({
            "from":    RESEND_FROM,
            "to":      [notify_email],
            "subject": subject,
            "html":    build_html(matches, store_id, store_name, FRONTEND_URL, account_email=account_email),
        })
        _log_email(notify_email, subject, ok=True, user=account_email)
        print(f"   Email sent â†’ {notify_email}")
    except Exception as exc:
        _log_email(notify_email, subject, ok=False, user=account_email, message=str(exc)[:200])
        print(f"   Email FAILED â†’ {notify_email}: {exc}")
        raise

# â”€â”€ Lambda handler â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def handler(event, context):
    """Lambda entry point â€” called by EventBridge weekly or manually.

    Supports three invocation modes:
      1. EventBridge scheduled rule  â†’ scrape + email all users
      2. Admin manual trigger        â†’ scrape + cache only (no email)
      3. User resend (source=manual-resend, target_email=<email>)
                                     â†’ scrape + email that one user only
    """
    source       = event.get("source", "")
    detail_type  = event.get("detail-type", "")
    target_email = (event.get("target_email") or "").strip().lower()

    # Explicit resend for a single user
    if source == "manual-resend" and target_email:
        main(send_emails=True, target_email=target_email)
        return {"statusCode": 200, "body": "Done (single-user resend)."}

    # Scheduled EventBridge trigger â†’ send to everyone
    send_emails = (source == "aws.events") or (detail_type == "Scheduled Event")
    main(send_emails=send_emails)
    return {"statusCode": 200, "body": "Done."}


def main(send_emails: bool = False, target_email: str = ""):
    started_at = datetime.now(timezone.utc)
    print(f"\nPublix Deal Checker â€” {date.today()}")
    print("=" * 50)

    job = {
        "job_id":          started_at.strftime("%Y%m%d-%H%M%S"),
        "started_at":      started_at.isoformat(),
        "finished_at":     "",
        "stores_examined": 0,
        "total_deals":     0,
        "emails_sent":     0,
        "errors":          [],
        "store_details":   [],
    }

    users = get_all_users()
    if target_email:
        users = [u for u in users if u.get("email", "").lower() == target_email]
        print(f"Single-user resend for: {target_email} ({len(users)} found)")
    else:
        print(f"Loaded {len(users)} registered user(s)")
    if not users:
        print("No users â€” nothing to do.")
        job["finished_at"] = datetime.now(timezone.utc).isoformat()
        write_scrape_log(job)
        return

    # Group users by store so we only fetch each store's deals once
    by_store: dict[str, list[dict]] = {}
    for u in users:
        sid = (u.get("prefs") or {}).get("store_id", "")
        if sid:
            by_store.setdefault(sid, []).append(u)

    if not by_store:
        print("No users have a store selected â€” nothing to do.")
        job["finished_at"] = datetime.now(timezone.utc).isoformat()
        write_scrape_log(job)
        return

    print(f"Stores to check: {list(by_store.keys())}\n")
    job["stores_examined"] = len(by_store)

    for store_id, store_users in by_store.items():
        prefs0     = store_users[0].get("prefs") or {}
        store_name = prefs0.get("store_name") or ""
        store_info = {"store_id": store_id, "store_name": store_name, "deals": 0, "emails": 0}
        print(f"â–¶ {store_name or f'Store #{store_id}'}")

        try:
            deals = get_deals(store_id)
            print(f"  {len(deals)} deals fetched")
            store_info["deals"] = len(deals)
            job["total_deals"] += len(deals)
        except Exception as e:
            msg = f"ERROR fetching deals for store {store_id}: {e}"
            print(f"  {msg}")
            job["errors"].append(msg)
            continue

        try:
            cache_deals(store_id, deals)
            print(f"  Deals cached to DynamoDB")
        except Exception as e:
            job["errors"].append(f"WARNING: cache failed for {store_id}: {e}")

        # â”€â”€ v6: write history snapshot and update corpus â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        try:
            week_start = _week_start(deals)
            history_deals(store_id, deals, week_start)
        except Exception as e:
            print(f"  WARNING: history write failed for {store_id}: {e}")
            job["errors"].append(f"WARNING: history write failed for {store_id}: {e}")

        try:
            update_corpus(deals)
        except Exception as e:
            print(f"  WARNING: corpus update failed: {e}")
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

        for user in store_users:
            account_email = user["email"]
            prefs         = user.get("prefs") or {}
            notify_email  = (prefs.get("notify_email") or account_email).strip()
            email_enabled = prefs.get("email_enabled", True)  # default True for existing users
            items         = prefs.get("items") or []
            matching      = prefs.get("matching") or {}

            print(f"  Matching for {account_email} ({len(items)} items)â€¦")

            if not items:
                print(f"    No items on list â€” skipping email")
                continue

            matches = match_deals_for_user(deals, items, matching)

            if not matches:
                print(f"    No matches this week")
                continue

            if not send_emails:
                print(f"    {len(matches)} match(es) found â€” skipping email (manual run)")
            elif not email_enabled:
                print(f"    {len(matches)} match(es) found â€” skipping email (user opted out)")
            else:
                print(f"    {len(matches)} match(es) â€” sending to {notify_email}")
                try:
                    send_alert(notify_email, matches, store_id, store_name, account_email=account_email)
                    job["emails_sent"] += 1
                    store_info["emails"] += 1
                except Exception as e:
                    msg = f"ERROR sending email to {notify_email}: {e}"
                    print(f"    {msg}")
                    job["errors"].append(msg)

        job["store_details"].append(store_info)

    job["finished_at"] = datetime.now(timezone.utc).isoformat()
    write_scrape_log(job)
    mode = "scheduled (emails sent)" if send_emails else "manual (no emails)"
    print(f"\nDone. Mode: {mode}. Emails sent: {job['emails_sent']}. Errors: {len(job['errors'])}")
